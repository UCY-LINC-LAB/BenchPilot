[{"id":0,"href":"/BenchPilot/docs/getting-started/","title":"Getting Started","section":"Docs","content":"BenchPilot #  BenchPilot: Repeatable \u0026amp; Reproducible Benchmarking for Edge Micro-DCs\nBenchPilot is a modular and highly customizable benchmarking framework for edge micro-DCs. BenchPilot provides a high-level declarative model for describing experiment testbeds and scenarios that automates the benchmarking process on Streaming Distributed Processing Engines (SDPEs). The latter enables users to focus on performance analysis instead of dealing with the complex and time-consuming setup. BenchPilot instantiates the underlying cluster, performs repeatable experimentation, and provides a unified monitoring stack in heterogeneous Micro-DCs.\nExperiment Setup #  A typical workflow starts with the user submitting in a yaml file their choice of experiments and their specific parameters. The BenchPilot model is composed with Experiments, where Workloads are described. Each workload can have the following:\n name, which will be selected from the supported workload list. record name, so that the user can later on retrieve its monitored metrics based on it number of repetitions, duration, specific workload parameters, cluster configurations including the manager node\u0026rsquo;s IP, list of cluster nodes, etc. engine configurations, in case of streaming distributed-based workloads  Deployment #  When the description is ready, the user deploys the application using the BenchPilotSDK through a Jupyter notebook. If there\u0026rsquo;s no validation error from the description, the Parser will parse the preferences to the BenchPilot Deployment Template Generator, where the preferences will be transformed into docker-compose templates. At last, the Deployment Coordinator will deploy each experiment to the underlying orchestrator and closely monitor its performance through the monitoring stack. At the beginning and end of each experiment, the Coordinator records the starting/ending timestamps, so that the user can retrieve the monitored information later on.\nMonitoring #  For extracting various infrastructure utilization metrics, including CPU, Memory, and Network Utilization, BenchPilot offers a transparent, from the application under test, monitoring stack. To achieve this, BenchPilot, in the bootstrapping stage, instantiates a containerized monitoring agent on every node. The agent inspects system information (e.g., performance pseudofiles and cgroup files) and extracts the required metrics in a non-intrusive way. The agent starts various probes, one for each sub-component (e.g., cgroup probe, OS probe, etc.), and exposes an API through which a centralized monitoring server retrieves the data periodically and stores them to the monitoring storage. Furthermore, the monitoring agent offers probes for external resources as well. From the implementation perspective, we have selected Netdata, a widely known and used monitoring tool, and Prometheus, an open-source and popular monitoring server, for our stack. For a monitoring storage backend, InfluxDB is used.\nPost-Experiment Analysis #  To create an end-to-end interactive analytic tool for benchmarking, BenchPilot utilizes the Jupyter Notebook stack. Specifically, after the experimentation process is over, the user can request the monitored metrics of each execution from the monitoring storage based on the provided experiments\u0026rsquo; starting/ending timestamps. Users can apply high-level analytic models to the retrieved metrics of each experiment and have a clear overview of their deployments. Workload List #  As for now BenchPilot only supports the following containerized workloads:\n   Name Description Specific Configuration Parameters     marketing-campaign A streaming distributed workload that features an application as a data processing pipeline with multiple and diverse steps that emulate insight extraction from marketing campaigns. The workload utilizes technologies such as Kafka and Redis. campaigns, which is the number of campaigns, the default number is 1000, tuples_per_second, the number of emitted tuples per second, the default is 10000 kafka_event_count, the number of generated and published events on kafka, the default is 1000000 maximize_data, this attribute is used to automatically maximize the data that are critically affecting the workload\u0026rsquo;s performance, the input that the user can put is in the format of x10, x100, etc.    It\u0026rsquo;s important to note that BenchPilot can be easily extended to add new workloads.\nResources #  The Team #  The creators of the BenchPilot are members of the Laboratory for Internet Computing (LInC), University of Cyprus. You can find more information about our research activity visit our publications\u0026rsquo; page and our on-going projects.\nAcknowledgements #  This work is partially supported by the EU Commission through RAINBOW 871403 (ICT-15-2019-2020) project and by the Cyprus Research and Innovation Foundation through COMPLEMENTARY/0916/0916/0171 project, and from RAIS (Real-time analytics for the Internet of Sports), Marie Sk≈Çodowska-Curie Innovative Training Networks (ITN), under grant agreement No 813162.\nLicense #  The framework is open-sourced under the Apache 2.0 License base. The codebase of the framework is maintained by the authors for academic research and is therefore provided \u0026ldquo;as is\u0026rdquo;.\nStart experimenting by installing Benchpilot now!\n"},{"id":1,"href":"/BenchPilot/docs/installation/","title":"Installation","section":"Docs","content":"GitHub Repository #  Visit our Github Repository  to download or fork BenchPilot!\nDockerHub #  All of our Docker images are uploaded on our DockerHub Repository!\nBenchPilot Bootstrapping #  Install Docker \u0026amp; Docker Compose #  In the \u0026lsquo;Getting Started\u0026rsquo; section we learned how BenchPilot is structured. In order to use the BenchPilot client you need to have installed docker \u0026amp; docker-compose. In our GitHub Repository, under the /utils folder we have prepared for you a script to automatically download it, so all you need to run is \u0026ldquo;sh install-docker.sh\u0026rdquo;.\nRetrieve or Build BenchPilot Client Image #  The second step is to either retrieve or build the BenchPilot client image.\nPulling Image from DockerHub #  For your ease, you can only pull the image from DockerHub just by running \u0026ldquo;docker pull benchpilot/benchpilot:client\u0026rdquo;.\nBuilding Image Locally #  If you want to build the image locally, you firstly need to download or clone our GitHub repository and then execute the building image script by running the command \u0026ldquo;sh build-image.sh\u0026rdquo;.\nStart BenchPilot Client and start experimenting! #  The final step is to just execute the following docker-compose.yaml by running the simple command \u0026ldquo;docker-compose up\u0026rdquo;. It is not needed from you to be familiarized with docker and docker-compose, but in case you want to learn more, you can always visit their website!\nversion: \u0026#39;3.8\u0026#39; # change it accordingly to your docker-compose version services:  benchpilot:  # if you chose to build it locally replace the it with: bench-pilot  image: benchpilot/benchpilot:client   volumes:  - /var/run/docker.sock:/var/run/docker.sock  - /usr/bin/docker:/usr/bin/docker  ports:  - 8888:8888 # port needed for jupyter  environment:  # define http(s)_proxy only if your devide is placed behind a proxy  - http_proxy=${http_proxy}  - https_proxy=${https_proxy}  # jupyter environment variables  - \u0026#34;JUPYTER_ENABLE_LAB=yes\u0026#34;  - \u0026#34;GRANT_SUDO=yes\u0026#34;  - \u0026#34;CHOWN_HOME=yes\u0026#34;  # Prometheus environment variables  - PROMETHEUS_IP=0.0.0.0  - PROMETHEUS_PREFIX=${your_datacenter_prefix}  user: root  # command to start jupyter  command: [\u0026#34;jupyter\u0026#34;, \u0026#34;notebook\u0026#34;, \u0026#34;--ip=\u0026#39;0.0.0.0\u0026#39;\u0026#34;, \u0026#34;--port=8888\u0026#34;, \u0026#34;--no-browser\u0026#34;, \u0026#34;--allow-root\u0026#34;, \u0026#34;--NotebookApp.token=\u0026#39;\u0026#39;\u0026#34;, \u0026#34;--NotebookApp.password=\u0026#39;\u0026#39;\u0026#34;] After starting the BenchPilot client, you can access jupyter through your browser from this link: \u0026ldquo;http://your_device_ip:8888\u0026rdquo;, and start experimenting!\nLearn how to define your experiments here.\n"},{"id":2,"href":"/BenchPilot/docs/experiments/","title":"Experiments","section":"Docs","content":"Experiments #  Before starting the benchmarking process, you need to define (i) the cluster under test, and (ii) the experiments you want to perform.\nDefining your Cluster #  By defining you cluster, Benchpilot removes the effort of you needing to download docker, docker compose and every docker image you may need for your benchmarking on your worker devices. All you need to do is define your cluster inside the /BenchPilotSDK/conf/bench-cluster.yaml.\nInside the \u0026ldquo;bench-cluster.yaml\u0026rdquo; we define our cluster nodes. So as we can see below, we define our cluster with the keywoard \u0026ldquo;cluster\u0026rdquo;, and afterwards we define each worker with the keyword \u0026ldquo;node\u0026rdquo;.\nFor each worker, it is needed to define their ip, hostname, and authentication credentials. These credentials include the username, and password or the path to an ssh key.\nOn each node there is also the option to skip the orchestration setup. For example, if in an experiment we use Docker Swarm as our orchestrator, and that node is already registered as a worker in our swarm, then it will skip the registration process.\ncluster:  - node:  ip: \u0026#34;xx.xx.xx.xx\u0026#34;  hostname: \u0026#34;raspberrypi0\u0026#34;  username: \u0026#34;your_username\u0026#34;  password: \u0026#34;your_password\u0026#34;  orchestrator_setup: \u0026#34;false\u0026#34;  - node:  ip: \u0026#34;xx.xx.xx.xx\u0026#34;  hostname: \u0026#34;server0\u0026#34;  username: \u0026#34;your_username\u0026#34;  ssh_key_path: \u0026#34;/BenchPilotSDK/conf/ssh_key.pem\u0026#34; # this is an example of ssh key path  .. ** Please keep in mind that every hostname and ip you declare needs to be accessible by the node you will run the BenchPilot client on.\nDefining your Experiments #  After defining our cluster, our final step before starting the benchmarking process is to define our experiments! All you need to is define them inside the /BenchPilotSDK/conf/bench-experimens.yaml file using the BenchPilot Model.\nThe BenchPilot model is composed with experiments, where workloads are described.\nFor each workload, you have to define the following:\n name, which will be selected from the supported workload list, e.g. \u0026ldquo;marketing-campaign\u0026rdquo;, record name, so that you can later on retrieve monitoring metrics based on it, number of repetitions, workload duration, specific workload parameters, cluster under test, including the control plane\u0026rsquo;s node public IP, and the list of the worker nodes\u0026rsquo; hostnames, engine configurations, in case of streaming distributed-based workloads  An example of the bench-experimens.yaml can be found below:\nexperiments:  - workload:  name: \u0026#34;marketing-campaign\u0026#34;  record_name: \u0026#34;storm_x1\u0026#34;  repetition: 1  duration: \u0026#34;10m\u0026#34;  parameters:  campaigns: 100  tuples_per_second: 1000  capacity_per_window: 10  time_divisor: 10000  cluster:  manager: \u0026#34;xx.xx.xx.xx\u0026#34; # control-plane node public ip  nodes: [ \u0026#34;raspberrypi0\u0026#34;, \u0026#34;server0\u0026#34; ] # workers\u0026#39; hostnames  # define http(s)_proxy only if your BenchPilot client is placed behind a proxy  http_proxy: \u0026#34;${http_proxy}\u0026#34;  https_proxy: \u0026#34;${https_proxy}\u0026#34;  engine:  name: \u0026#34;storm\u0026#34;  parameters:  partitions: 5  ackers: 2  - workload:  name: \u0026#34;marketing-campaign\u0026#34;  record_name: \u0026#34;flink_x1\u0026#34;  repetition: 5  duration: \u0026#34;30m\u0026#34;  .. ** BenchPilot will run the experiments with the order you described them.\n"},{"id":3,"href":"/BenchPilot/docs/workloads/","title":"Workloads","section":"Docs","content":"Workloads #  As for now BenchPilot only supports the following containerized workloads:\n   Name Description Specific Configuration Parameters     marketing-campaign A streaming distributed workload that features an application as a data processing pipeline with multiple and diverse steps that emulate insight extraction from marketing campaigns. The workload utilizes technologies such as Kafka and Redis. campaigns, which is the number of campaigns, the default number is 1000, tuples_per_second, the number of emitted tuples per second, the default is 10000 kafka_event_count, the number of generated and published events on kafka, the default is 1000000 maximize_data, this attribute is used to automatically maximize the data that are critically affecting the workload\u0026rsquo;s performance, the input that the user can put is in the format of x10, x100, etc.    Distributed Processing Engine Parameters #  In the case of streaming distributed workloads, the user needs to define specific engine parameters in their experiments.\nFor each Streaming Distributed Processing Engine, the following attributes can be specified:\nEngine Storm Flink Spark   Parameters partitionsackers partitionsbuffer_timeoutcheckpoint_interval partitionsbatchtimeexecutor_coresexecutor_memory   It\u0026rsquo;s important to note that BenchPilot can be easily extended to add new workloads.\nFor extending BenchPilot check this section out.\n"},{"id":4,"href":"/BenchPilot/docs/monitoring/","title":"Monitoring","section":"Docs","content":"BenchPilot Monitoring System #  BenchPilot Monitoring divides its services on the Control Plane, where BenchPilot Client and core monitoring services will run on, and its workers, where the benchmarking will happen.\nOn the Control Plane, BenchPilot reuses the following existing projects:\n Consul, for service registration Prometheus, for keeping metrics Influxdb, for long-term storing metrics  On each Worker device, BenchPilot uses Netdata for capturing its metrics, and meross smart plugs for retrieving energy consumption. You can use any smart plug you wish for capturing energy consumption by exposing its measures to Netdata.\nEvery file that you may need for the monitoring stack, you can find in our repository, under the \u0026ldquo;monitoring\u0026rdquo; folder.\nSetup Monitoring System #  First of all you need to download or clone our GitHub Repository.\nInstall Docker \u0026amp; Docker Compose #  If you haven\u0026rsquo;t installed docker and docker-compose on your devices yet (control plane \u0026amp; workers), just execute the following command on each one of them:\nsh install-docker.sh Start Control-Plane Services #  When you have docker and docker-compose installed, all you need to run on the Control Plane the following command:\ndocker-compose up -f docker-compose-monitoring.yaml Don\u0026rsquo;t forget to replace the environment variables however you would wish to (e.g. \u0026ldquo;${database_name}\u0026rdquo;)\nStart Worker Services #  On each Worker you can start and setup Netdata by just running:\ndocker-compose up -f docker-compose.yaml Keep in mind, that you need to define the smart plug\u0026rsquo;s ip.\nChanging smart plug configuration #  There are two things needed for this process:\n First you should remove/change the \u0026ldquo;smart-plug\u0026rdquo; service from docker-compose.yaml Update the netdata/prometheus.conf. You should update accordingly the \u0026ldquo;smart_plug\u0026rdquo; configuration, which is defined in the end of the \u0026ldquo;prometheus.conf\u0026rdquo;. This setup if for exposing its metrics to netdata.  Register Worker nodes to Consul #  On each worker, after you have started the worker services, just enter the Consul folder and execute the following command:\nsh consul/register_to_consul.sh Please, don\u0026rsquo;t forget to replace the IPs in the script with your IPs (Consul Ip, and worker device IP)\n"},{"id":5,"href":"/BenchPilot/docs/experiment-analysis/","title":"Post-Experiment Analysis","section":"Docs","content":"Post-Experiment Analysis #  After running all of our experiments, it\u0026rsquo;s time to view our results!\nJupyter #  For your ease of accessing and processing the results, we used Jupyter Notebook.\nPrometheus Client #  We have created under /utils package a prometheus client. You can always retrieve your experiment results by calling the \u0026ldquo;get_benchmark_results\u0026rdquo; method, or export them in a csv format with the \u0026ldquo;xport_results_to_csv\u0026rdquo; method.\nPlease remember to use the methods \u0026ldquo;assign_prometheus_prefix\u0026rdquo; and \u0026ldquo;assign_prometheus_suffix\u0026rdquo; in order to assign your prometheus\u0026rsquo; prefixes and suffixes.\nFor more information about Jupyter, please visit their website.\n"},{"id":6,"href":"/BenchPilot/docs/exteding-framework/","title":"Exteding BenchPilot","section":"Docs","content":"Extending BenchPilot #  This section describes all of the steps you need to take in order to extend BenchPilot for supporting more workloads!\nDockerize Workload #  The first step to extend BenchPiot, is to create the necessary docker images. In general BenchPilot utilizes the idea of having a controller node (which BenchPilot\u0026rsquo;s client and other core services will reside on), and the workers, which will be the system under test. Having this scheme in mind, you need to dockerize your workload, and to divide it into images that will reside on your controller node, and another image which will be deployed on the workers.\nAdding New Services #  After creating the latter images, you should add under the /BenchPilotSDK/services the new service. That class should derive its properties from the BenchPilot\u0026rsquo;s abstract service object. Keep in mind that for every docker image you created for your workload, you should declare it as a different service.\nFor each service it is important to declare the following:\n docker image, either an already existing one, or you have to create it on your own hostname, we use the same one as the service name usually image tag image arm tag, if one exists ports, needed ports environment, needed environment variables / configurations service log, the log that the service prints when is up and running Depends On, here you should add the service name that it\u0026rsquo;s important to start before the one you just created Command, in case if it needs to execute a specific command when the service starts Proxy, a simple \u0026ldquo;True\u0026rdquo; / \u0026ldquo;False\u0026rdquo; definition, whether it will reside on a device that passes through proxy needs_placement, again, \u0026ldquo;True\u0026rdquo; if it should reside on a worker, \u0026ldquo;False\u0026rdquo; if it\u0026rsquo;s a core service and will reside on the manager node  Below you can see a service example:\nfrom BenchPilotSDK.services.service import Service   class Redis(Service):  \u0026#34;\u0026#34;\u0026#34; This class represents the redis docker service \u0026#34;\u0026#34;\u0026#34;   def __init__(self):  Service.__init__(self)  self.hostname = \u0026#34;redis\u0026#34;  self.image = \u0026#34;bitnami/redis\u0026#34;  self.image_tag = \u0026#34;6.0.10\u0026#34;  self.image_arm_tag = \u0026#34;not supported\u0026#34; # in case of not supporting arm images  self.ports = [\u0026#34;6379:6379\u0026#34;]  self.environment = [\u0026#34;ALLOW_EMPTY_PASSWORD=yes\u0026#34;]  self.service_log = \u0026#34;Ready to accept connections\u0026#34; ** Before adding new services, check first if it already exists.\nAdding New Workload #  After adding all of your workload\u0026rsquo;s services, you should create a new workload class as well, under the /BenchPilotSDK/workloads. This particular class will inherit its behavior from the \u0026ldquo;workload\u0026rdquo; class. In that class you should add in the \u0026ldquo;services list\u0026rdquo; all the services you need.\nIn the following block you can find a Workload example:\nfrom abc import ABC  from BenchPilotSDK.services.sdpe.engines import * from BenchPilotSDK.services.sdpe.engine import Engine from BenchPilotSDK.workloads.workload import Workload   class SDPEWorkload(Workload, ABC):  \u0026#34;\u0026#34;\u0026#34; This class extends workload and adds the assign engine method in order to be able to assign a specific streaming distributed processing engine \u0026#34;\u0026#34;\u0026#34;  engine: Engine   def __init__(self, workload_yaml, workload_name: str):  super().__init__(workload_yaml)  self.workload_setup.check_required_parameters(\u0026#39;workload\u0026#39;, [\u0026#34;engine\u0026#34;], workload_yaml)  self.workload_setup.check_required_parameters(\u0026#39;engine\u0026#39;, [\u0026#34;name\u0026#34;], workload_yaml[\u0026#34;engine\u0026#34;])  self.assign_engine(workload_yaml[\u0026#34;engine\u0026#34;][\u0026#34;name\u0026#34;], workload_name)  self.services.append(self.engine)   # this method is for generalizing the term \u0026#34;Engine\u0026#34;, hence the Engine is a generalized service that can be either Storm, Flink or Spark  def assign_engine(self, engine: str, workload_name: str):  if not engine is None:  if \u0026#34;spark\u0026#34; in engine.lower():  self.engine = Spark(len(self.nodes), self.manager_ip, workload_name)  elif \u0026#34;storm\u0026#34; in engine.lower():  self.engine = Storm(len(self.nodes), self.manager_ip, workload_name)  elif \u0026#34;flink\u0026#34; in engine.lower():  self.engine = Flink(len(self.nodes), self.manager_ip, workload_name) Adding a new SDPE Workload #  In case of adding a new Streaming Distributed - based workload you don\u0026rsquo;t need to add the engines, you only need to inherit from the SDPEWorkload class, and add the rest of the services, like the example below:\nfrom abc import ABC  from BenchPilotSDK.workloads.sdpeWorkload import SDPEWorkload from BenchPilotSDK.services.kafka import Kafka from BenchPilotSDK.services.redis import Redis from BenchPilotSDK.services.zookeeper import Zookeeper from BenchPilotSDK.workloads.setup.yahooWorkloadSetup import YahooWorkloadSetup  # Inherits from the SDPEWorkload class that already exists class Yahoo(SDPEWorkload, ABC):  \u0026#34;\u0026#34;\u0026#34; This class represents Yahoo Streaming Benchmark, it holds all of the extra needed services. - by extra we mean the services that are not DSPEs \u0026#34;\u0026#34;\u0026#34;   def __init__(self, workload_yaml):  super().__init__(workload_yaml, \u0026#34;marketing-campaign\u0026#34;)  self.services.append(Zookeeper())  kafka = Kafka(len(self.nodes), self.manager_ip)  self.services.append(kafka)  # defines which services would need to restart when a trials is over  self.restarting_services.append(kafka)  redis = Redis()  self.services.append(redis)  self.restarting_services.append(redis)  # Assigns a workload setup  self.workload_setup = YahooWorkloadSetup(workload_yaml) Creating a workload Setup #  Under the /workloads we also included a setup package. In the setup package, you can add a new setup class thay you may need for configuring your workload. Your class should inherit and override the following class:\nimport os from abc import abstractmethod  from BenchPilotSDK.utils.exceptions import MissingBenchExperimentAttributeException   class WorkloadSetup(object):  \u0026#34;\u0026#34;\u0026#34; The workloadSetup is responsible to set up the needed configuration files of a workload \u0026#34;\u0026#34;\u0026#34;  cluster_dockerfile_path: str = os.environ[\u0026#34;BENCHPILOT_PATH\u0026#34;]  workload_client_path: str = os.environ[\u0026#34;BENCHPILOT_PATH\u0026#34;] + \u0026#34;dockerized-benchmarks/workload-client/\u0026#34;   def __init__(self, workload_yaml = None):  self.workload_yaml = workload_yaml   @abstractmethod  def setup(self, parameters = None):  self.update_workload_configuration(parameters)   @abstractmethod  def update_workload_configuration(self, parameters):  # TODO override depending on the workload when rebuilding image  pass   @staticmethod  def check_required_parameters(main_attribute, required_attributes, yaml):  for att in required_attributes:  if not att in yaml:  raise MissingBenchExperimentAttributeException(main_attribute + \u0026#34;\u0026gt; \u0026#34; + att) In case of not needing a setup, you can just ignore this step.\nAdding the workload into the workload factory #  After adding the new workload, you should add a new \u0026ldquo;if\u0026rdquo; statement in the workload factory. This factory is applying the factory design pattern, and was added in order to generalize the \u0026ldquo;workload\u0026rdquo; object without needing to know with which object we are dealing with.\nBelow you can find the code from the /BenchPilotSDK/workloads/WorkloadFactory.py, where you need to replace in the last lines your new workload\u0026rsquo;s name:\nfrom BenchPilotSDK.workloads import * from BenchPilotSDK.workloads.setup.workloadSetup import WorkloadSetup  class WorkloadFactory:  \u0026#34;\u0026#34;\u0026#34; This class works as a workload factory. \u0026#34;\u0026#34;\u0026#34;  workload: Workload  workloadSetup: WorkloadSetup   def __init__(self, workload_yaml):  workloadSetup = WorkloadSetup()  workloadSetup.check_required_parameters(\u0026#39;workload\u0026#39;, [\u0026#34;name\u0026#34;], workload_yaml)  workload_name = workload_yaml[\u0026#34;name\u0026#34;].lower()  if \u0026#34;yahoo\u0026#34; in workload_name or \u0026#34;marketing-campaign\u0026#34; in workload_name:  self.workload = Yahoo(workload_yaml)  # replace the following lines with your new workload name  elif \u0026#34;your-new-workload-name\u0026#34; in workload_name:  self.workload = YourNewWorkloadName(workload_yaml) Creating a new agent for submitting the job #  This part may be the trickiest one. You need to create a new agent. For the record, the agent is basically a Flask REST API which will get every now and again requests from BenchPilot to (i) start a new benchmark, (ii) check the benchmarking status (if it began, if it\u0026rsquo;s running or if it has finished), and (iii) to force stop a running benchmark. So you need to implement these three functionalities in order for BenchPilot to work as expected.\nThis class exists under the following path: dockerized-benchmarks/workload-client/src.\n"}]